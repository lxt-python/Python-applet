from fake_useragent import UserAgent
import urllib
from urllib import request 
from  bs4 import BeautifulSoup  
import re  
import time
import os
import random  

ua = UserAgent()  
url = "https://www.zhihu.com/question/22918070"

proxies=({'http': '1.197.178.38:61234'},
         {'http': '119.136.90.125:808'},
         {'http': '14.118.252.254:6666'},
         {'http': '123.53.118.122:61234'},
         {'http': '113.86.221.214:808'},
         {'http': '1.196.63.35:61234'},
         {'http': '14.118.255.212:6666'},
         {'http': '14.118.252.3:6666'})
flag=0

while flag==0:  
    proxy = random.choice(proxies)
    proxy_support =urllib.request.ProxyHandler(proxy)
    #创建Opener
    opener =urllib.request.build_opener(proxy_support)
    #添加User Angent
    opener.addheaders = [('User-Agent',ua.random)]
    #安装OPener
    urllib.request.install_opener(opener)
    try:
        #使用自己安装好的Opener
        response = urllib.request.urlopen(url)
        flag=1
    except:
        flag=0
#读取相应信息并解码
html = response.read().decode("utf-8") 
soup = BeautifulSoup(html,'html.parser')  
#print(soup.prettify())  
  
#用Beautiful Soup结合正则表达式来提取包含所有图片链接（img标签中，class=**，以.jpg结尾的链接）的语句  
links = soup.find_all('img', "origin_image zh-lightbox-thumb",src=re.compile(r'.jpg$'))  
print(links)  
  
# 设置保存图片的路径，否则会保存到程序当前路径  
path = ('.\images')
if not os.path.exists(path):
    os.makedirs(path)                            #路径前的r是保持字符串原始值的意思，就是说不对其中的符号进行转义  
for link in links:  
    print(link.attrs['src'])  
    #保存链接并命名，time.time()返回当前时间戳防止命名冲突  
    request.urlretrieve(link.attrs['src'],path+'\%s.jpg' % time.time())  #使用request.urlretrieve直接将所有远程链接数据下载到本地 
